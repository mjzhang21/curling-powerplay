---
title: "CSAS 2026: First Shot Clustering on Preprint Dataset"
author: "Mark Zhang"
date: last-modified
format:
  pdf:
    colorlinks: true
    linkcolor: blue
    number-sections: true
    geometry: margin=1in
    fontsize: 11pt
---

## Data Preprocessing

Import the data.
```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df_games = pd.read_csv("../data/mxd_games.csv")
df_shots = pd.read_csv(
    "../data/mxd_shots.csv", 
    dtype={60: str, 64: str}
)
df_games.head()
```

Make all the columns lowercase.
```{python}
df_shots.columns = df_shots.columns.str.replace(r'[A-Z]', lambda m: m.group(0).lower(), regex=True)
df_shots.head()
```

Create a function that plots a proportional curling sheet containing circles,
back line and hogline.
```{python}
def plot_curling_sheet():
    """
    Plots a simple curling sheet with house circles only.
    """
    # Sheet coordinates and house radii
    CENTER_X, BUTTON_Y, BACK_Y, HOG_Y = 150, 160, 40, 580
    radii = [120, 80, 40, 10]  # house circles

    fig, ax = plt.subplots(figsize=(4, 8))

    # Draw sheet lines
    ax.axvline(CENTER_X, linestyle='--', linewidth=1)
    ax.axhline(BUTTON_Y, linestyle='--', linewidth=1)
    ax.axhline(BACK_Y, linewidth=1)
    ax.axhline(HOG_Y, linewidth=1)

    # Draw house circles
    for r in radii:
        ax.add_patch(plt.Circle((CENTER_X, BUTTON_Y), r, fill=False, color='black'))

    # Formatting
    ax.set_aspect('equal')
    ax.set_xlim(0, 300)
    ax.set_ylim(0, 600)
    ax.set_title('Curling Sheet')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    return ax

```

Filter the data to the board position after first shot of the left formation powerplay.
```{python}
df_ppl = df_shots[df_shots['left_pp'] == 1]
df_ppl_fs = df_ppl[df_ppl['shot'].isin([1])]

```

View values for the first 2 stone positions to see how the data is organized.
```{python}
  # List of column pairs
pairs = [['r1x', 'r1y'], ['y1x', 'y1y'], ['r2x', 'r2y'], ['y2x', 'y2y'],['shooterx', 'shootery']]

# Compute value counts (with percentages) for each pair
for pair in pairs:
    counts = df_ppl_fs[pair].value_counts(normalize=True) * 100  
    print(f"Value counts (percent) for {pair}:\n{counts.round(2)}\n") 

```
Preplaced stone coordinates in the left powerplay formation: 
Hammer/powerplay team: ~(70, 170)
Non-hammer team: ~(82, 383) 

Stones with a y value of ~10 are out of play/knocked out. They are in front the hog line.
Stones with a y value of ~589 are not yet played. They are behind the back line.

```{python}
plot_curling_sheet()
plt.scatter(df_ppl_fs['r1x'], df_ppl_fs['r1y'], alpha=0.5, label='R1', s=60)
plt.scatter(df_ppl_fs['y1x'], df_ppl_fs['y1y'], alpha=0.5, label='Y1', s=60)
plt.legend()
plt.show()
```
From the printed value counts and visually,
R1 and Y1 seem to have values for both the hammer stone and the non-hammer stone.

```{python}
plot_curling_sheet()
plt.scatter(df_ppl_fs['r2x'], df_ppl_fs['r2y'], alpha=0.5, label='R2', s=60)
plt.scatter(df_ppl_fs['y2x'], df_ppl_fs['y2y'], alpha=0.5, label='Y2', s=60)
plt.legend()
plt.show()
```
R2 and Y2 are both the non-hammer team guard.

```{python}
plot_curling_sheet()
plt.scatter(df_ppl_fs['shooterx'], df_ppl_fs['shootery'],alpha=0.5, label='shooter',s=60)
plt.legend()
plt.show()
```
The shoter stone is not played yet.

Looking at the data, red and yellow stones do not relate to other variables besides the color assignment for the team. Also, the columns sometimes contain stones from the hammer and non-hammer team. I aim to process the data so each column represents the stone coordinates for each team's corresponding stone. 

First lets rename variables to indicate the hammer/ non-hammer team rather than team 1 or team 2.
```{python}
df = df_ppl_fs[['id', 'game', 't1','t2','end','hammer_end','shooterx','shootery',
                'shot', 'throw','turn','percent','gender','handedness',
                'r1x','r1y','y1x','y1y','r2x','r2y','y2x','y2y',
                't1_has_pp_available','t2_has_pp_available',
                't1before','t1during','t1after',
                't2before','t2during','t2after',
                ]]
df = df.rename(columns={
    'r1x': 'nh0x',
    'r1y': 'nh0y',
    'y1x': 'h0x',
    'y1y': 'h0y',
    'r2x': 'nh1x',
    'r2y': 'nh1y',
    'hammer_end': 'hteam',
})
```

```{python}
# hammer mask
is_t1_hammer = df['t1'] == df['hteam']

# non-hammer team id
df['nhteam'] = np.where(is_t1_hammer, df['t2'], df['t1'])

# score states
for phase in ['before', 'during', 'after']:
    df[f'h_{phase}']  = np.where(is_t1_hammer,
                                        df[f't1{phase}'],
                                        df[f't2{phase}'])
    df[f'nh_{phase}'] = np.where(is_t1_hammer,
                                        df[f't2{phase}'],
                                        df[f't1{phase}'])

# power play
df['h_has_pp_available']  = np.where(
    is_t1_hammer,
    df['t1_has_pp_available'],
    df['t2_has_pp_available']
)
df['nh_has_pp_available'] = np.where(
    is_t1_hammer,
    df['t2_has_pp_available'],
    df['t1_has_pp_available']
)

df['score_diff_before'] = df['h_before'] - df['nh_before']
```
We swap the preplaced stones and shift the other shots to make the stone column
align with shot number.

```{python}
df['nh1x'] = df['nh1x'].fillna(df['y2x'])
df['nh1y'] = df['nh1y'].fillna(df['y2y'])

mask1 = df['nh0x'].between(69, 72) & df['nh0y'].between(167, 171)
mask2 = df['h0x'].between(78, 84) & df['h0y'].between(365, 385)

swap_mask = mask1 | mask2

cols_nh = ['nh0x', 'nh0y']
cols_h = ['h0x', 'h0y']

df.loc[swap_mask, cols_nh + cols_h] = (
    df.loc[swap_mask, cols_h + cols_nh].values
)
swap_mask.sum()

```

```{python}
non_matches = ~((df["nh1x"] == df["shooterx"]) & (df["nh1y"] == df["shootery"]))
count = non_matches.sum()
print(count)
```

```{python}
df.loc[non_matches, ["nh1x", "nh1y", "nh0x", "nh0y"]] = \
    df.loc[non_matches, ["nh0x", "nh0y", "nh1x", "nh1y"]].values
non_matches = ~((df["nh1x"] == df["shooterx"]) & (df["nh1y"] == df["shootery"]))
count = non_matches.sum()
print(count)
```

```{python}

df.loc[non_matches, ["nh1x", "nh1y", "h0x", "h0y"]] = \
    df.loc[non_matches, ["h0x", "h0y", "nh1x", "nh1y"]].values
non_matches = ~((df["nh1x"] == df["shooterx"]) & (df["nh1y"] == df["shootery"]))
count = non_matches.sum()
print(count)

```

```{python}
mask1 = df['nh0x'].between(69, 72) & df['nh0y'].between(167, 171)
mask2 = df['h0x'].between(78, 84) & df['h0y'].between(365, 385)

swap_mask = mask1 | mask2

cols_nh = ['nh0x', 'nh0y']
cols_h = ['h0x', 'h0y']

df.loc[swap_mask, cols_nh + cols_h] = (
    df.loc[swap_mask, cols_h + cols_nh].values
)
swap_mask.sum()

```
```{python}
df = df.drop(columns=[
    't1before','t1during','t1after',
    't2before','t2during','t2after',
    't1_has_pp_available','t2_has_pp_available',
    't1','t2', 'y2x','y2y'
],errors='ignore')

```

```{python}
plot_curling_sheet()
plt.scatter(df['nh0x'], df['nh0y'], alpha=0.5, label='nh0', s=60)
plt.scatter(df['h0x'], df['h0y'], alpha=0.5, label='h0', s=60)
plt.scatter(df['nh1x'], df['nh1y'], alpha=0.5, label='nh1', s=60)
plt.legend()
plt.show()
```


## Clustering
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score
from scipy.spatial.distance import cdist

# Numeric features only
numeric_features = ['nh0x','nh0y','h0x','h0y','nh1x','nh1y']
#categorical_features = ['turn','throw',]

X_scaled = pd.DataFrame(
    StandardScaler().fit_transform(df[numeric_features]),
    columns=numeric_features,
    index=df.index
)

# # One-hot encode categorical features
# X_cat = pd.get_dummies(df[categorical_features], drop_first=False)

# # Combine
# X_scaled = pd.concat([X_num_scaled, X_cat], axis=1)
```

```{python}
pca_full = PCA()  # fit all components
X_pca_full = pca_full.fit_transform(X_scaled)

explained_variance = pca_full.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Plot variance contributions
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(np.arange(1, len(explained_variance)+1), explained_variance, marker='o')
axes[0].set_xlabel('Principal Component')
axes[0].set_ylabel('Explained Variance Ratio')
axes[0].set_title('Variance by Component')

axes[1].plot(np.arange(1, len(cumulative_variance)+1), cumulative_variance, marker='o')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('Cumulative Explained Variance')
axes[1].set_title('Cumulative Variance')
plt.tight_layout()
plt.show()
```

```{python}
pca_2 = PCA(n_components=2)
X_pca_2 = pca_2.fit_transform(X_scaled)

loadings = pd.DataFrame(
    pca_2.components_.T,
    columns=[f'PC{i+1}' for i in range(pca_2.n_components_)],
    index=X_scaled.columns
)

pd.options.display.float_format = '{:.3f}'.format
print("Top contributors to PC1:")
print(loadings['PC1'].abs().sort_values(ascending=False))
print("\nTop contributors to PC2:")
print(loadings['PC2'].abs().sort_values(ascending=False))

print("\nExplained variance ratio per PC:")
print(pca_2.explained_variance_ratio_)
```

```{python}
plt.figure(figsize=(8,6))
plt.scatter(X_pca_2[:,0], X_pca_2[:,1], alpha=0.6)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Projection of Features')
plt.show()
```
```{python}
# Range of cluster numbers to try
k_values = range(1, 15)
inertia = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot
plt.figure(figsize=(6,4))
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia (Sum of squared distances)')
plt.title('Elbow Method to Choose k')
plt.xticks(k_values)
plt.show()

```
```{python}
sil_scores = []
K_range = range(2, 11)

for k in K_range:
    km = KMeans(
        n_clusters=k,
        n_init=10,
        random_state=42
    ).fit(X_scaled)  # use scaled data
    sil = silhouette_score(X_scaled, km.labels_)
    sil_scores.append(sil)

# Find best K
best_k = K_range[np.argmax(sil_scores)]
print(f"Best K by silhouette score: {best_k}")

# Print scores
print("Silhouette Scores by K:")
for k, s in zip(K_range, sil_scores):
    print(f"K = {k}: {s:.3f}")

# Plot
plt.figure(figsize=(6,4))
plt.plot(list(K_range), sil_scores, marker="o")
plt.title("Silhouette Score vs K")
plt.xlabel("Number of clusters (K)")
plt.ylabel("Silhouette Score")

# Mark the best K
plt.axvline(best_k, color="tab:red", ls="--", label=f"Best K â‰ˆ {best_k}")
plt.scatter(best_k, max(sil_scores), color="red", s=80, zorder=5)

# Optional: annotate best K
plt.text(best_k + 0.2, max(sil_scores)-0.02, f"K={best_k}", color="black", fontsize=10)

plt.legend()
plt.grid(alpha=0.3)
plt.show()

```

```{python}
k = 3  # choose based on elbow method
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X_scaled)

# Visualize clusters in PCA space
plt.figure(figsize=(8,6))
plt.scatter(X_pca_2[:,0], X_pca_2[:,1], c=labels, cmap='tab10', alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title(f'KMeans Clusters (k={k}) Projected on PCA')
plt.colorbar(label='Cluster')
plt.show()

```
```{python}
tsne = TSNE(
    n_components=2,
    perplexity=20,
    learning_rate='auto',
    init='random',
    random_state=42
)
X_tsne = tsne.fit_transform(X_scaled)

plt.figure(figsize=(8, 4))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')
plt.title("Clusters visualized with t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.show()
```

```{python}
sil_score = silhouette_score(X_scaled, labels)
db_score = davies_bouldin_score(X_scaled, labels)
ch_score = calinski_harabasz_score(X_scaled, labels)

print(f"Silhouette Score: {sil_score:.3f}")
print(f"Davies-Bouldin Score: {db_score:.3f}")
print(f"Calinski-Harabasz Score: {ch_score:.3f}")

```



```{python}
import hdbscan
import numpy as np
import pandas as pd
from itertools import product
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Define the parameter grid
min_cluster_sizes = range(5,10)     # 5,10,15,20,25,30
min_samples_list = range(1,10)

results = []

for mcs, ms in product(min_cluster_sizes, min_samples_list):
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=mcs,
        min_samples=ms,
        gen_min_span_tree=True
    )
    clusters = clusterer.fit_predict(X_scaled)

    # Skip if no clusters formed
    if np.all(clusters == -1):
        continue

    coverage = np.sum(clusters != -1) / X_scaled.shape[0]
    dbcv = clusterer.relative_validity_

    results.append({
        'min_cluster_size': mcs,
        'min_samples': ms,
        'coverage': coverage,
        'dbcv': dbcv,
        'n_clusters': len(set(clusters)) - (1 if -1 in clusters else 0)
    })

# Convert to DataFrame
df_results = pd.DataFrame(results)

# Sort by DBCV descending
df_results = df_results.sort_values('dbcv', ascending=False).reset_index(drop=True)
print(df_results)
```
```{python}
import hdbscan
import pandas as pd
import numpy as np


# Example: best params from your grid search
best_min_cluster_size = 6
best_min_samples = 4

# Fit HDBSCAN
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=best_min_cluster_size,
    min_samples=best_min_samples,
    gen_min_span_tree=True
)

clusters = clusterer.fit_predict(X_scaled)  # X_scaled is your preprocessed/scaled feature matrix

# Add cluster labels to your DataFrame
df['hdbscan_cluster'] = clusters

# Cluster sizes
print(df['hdbscan_cluster'].value_counts())

# How many points were considered noise (-1)
num_noise = (df['hdbscan_cluster'] == -1).sum()
print(f"Noise points: {num_noise}")

# Cluster membership probabilities (optional)
df['membership_prob'] = clusterer.probabilities_

# Example: stats per cluster
cluster_stats = (
    df[df['hdbscan_cluster'] != -1]
    .groupby('hdbscan_cluster')['membership_prob']
    .agg(['mean', 'median', 'min', 'max', 'count'])
)
print(cluster_stats)

```
```{python}
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 4))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
plt.title("Clusters visualized with PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(
    *scatter.legend_elements(),
    title="Cluster"
)
plt.show()
```

```{python}
tsne = TSNE(
    n_components=2,
    perplexity=50,
    learning_rate='auto',
    init='random',
    random_state=42
)
X_tsne = tsne.fit_transform(X_scaled)

plt.figure(figsize=(8, 4))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='viridis')
plt.title("Clusters visualized with t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.legend(
    *scatter.legend_elements(),
    title="Cluster"
)

plt.show()

```

```{python}
mask = clusters != -1
sil_score = silhouette_score(X_scaled[mask], clusters[mask])
db_score = davies_bouldin_score(X_scaled[mask], clusters[mask])
ch_score = calinski_harabasz_score(X_scaled[mask], clusters[mask])

print(f"Silhouette Score: {sil_score}")

print(f"Davies-Bouldin Index: {db_score}")

print(f"Calinski-Harabasz Index: {ch_score}")

```